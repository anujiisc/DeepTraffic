
![Logo](data:image/png;base64,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)




The DeepTraffic repository encompasses object detection and object tracking folders, featuring various algorithms and resources for traffic flow prediction and optimization.

# Object Detection


working .....



# Object Tracking 

We delve into two multi-object tracking algorithms: FairMOT and YOLOv8 with ByteTrack. Beginning with pre-trained COCO models, we trained our models on the Indian Driving Dataset (IDD) and subsequently tested them on the Gram Dataset. Our evaluation includes an in-depth analysis of performance metrics and insights gained from this experimentation process.


## 1. YOLOv8_ByteTrack

### Data Preparation 

Dataset should be in following format.

GRAM Dataset

```bash
Object Tracking/
    |-- YOLOv8_ByteTrack/
        |-- GRAM/
            |-- GRAM-RTMv4/
            |-- images/
                |-- M-30/
                    |-- 1.jpg
                    ...
                |-- M-30-HD/
                    |-- 1.jpg
                    ...
                |-- Urban1/
                    |-- 1.jpg
                    ...
            |-- labels/
                |-- M-30/
                    |-- 1.txt
                    ...
                |-- M-30-HD/
                    |-- 1.txt
                    ...
                |-- Urban1/
                    |-- 1.txt
                    ...
        
            |-- test.txt
            |-- train.txt
            |-- val.txt
            |-- data.ipynb

    |-- YOLOv8_ByteTrack
    |-- FairMOT

  ```

### Installation & Run Code


In order to use this repository, you need to create an environment:
1. Clone the github repository:
```bash
git clone https://github.com/anujiisc/DeepTraffic.git
```
2. Now you need to create a conda environment:
```bash
cd Object\ Tracking/
cd YOLOv8_ByteTrack/
conda env create -f environment.yml
conda activate yolov8
```
3. Train model using following command:
```bash
python train.py --batch 16 --data gram.yaml --pretrained_weights yolov8n.pt \
--device 1,2 --epoch 200 --img_size 1280 --save_results yolov8n_gram
```
You can adjust these arguments as required. The results and model weights are saved in directory
runs/detect/yolov8n gram. 

4. Track videos using trained model using following command:
```bash
python track.py --trained_weights ./runs/detect/yolov8s_gram/weights/best.pt \
--video_path ./videos/M-30.mp4 --save_results ./results/M-30.txt \
--type_tracker bytetrack.yaml
```
You can adjust these arguments as required. Tracking results will be saved in directory results/M-30.txt as in format required for MOT challenge.

5. Calculate evaluation metrics on tracked results received from previous command:
python eval.py
Actual MOT challenge format files are inside GRAM MOT/ and results files are inside results/. The code is available in mot format.ipynb . Tracking code is also available in this file.

### Performance on GRAM Dataset

    
[Documentation](https://linktodocumentation)



## 2. FairMOT


### Data Preparation 

Dataset should be in following format.

Indian Driving Dataset

```bash
Object Tracking/
    |-- FairMOT/
        |-- IDD/
            |-- images/
                |-- train/
                    |-- 1.jpg
                    |-- 2.jpg
                    ...
                |-- val/
                    |-- 1.jpg
                    |-- 2.jpg
                    ...
            |-- labels_with_ids/
                |-- train/
                    |-- 1.txt
                    |-- 2.txt
                    ...
                |-- val/
                    |-- 1.txt
                    |-- 2.txt
                    ...   

  ```

GRAM Dataset
  
```bash
Object Tracking/
    |-- FairMOT/
        |-- GRAM/
            |-- images/
                |-- test/
                    |-- M-30/
                        |-- 1.jpg
                        ...
                    |-- M-30-HD/
                        |-- 1.jpg
                        ...
                    |-- Urban1/
                        |-- 1.jpg
                        ...
                |-- train/
                    |-- M-30/
                        |-- 1001.jpg
                        ...
                    |-- M-30-HD/
                        |-- 1001.jpg
                        ...
                    |-- Urban1/
                        |-- 1001.jpg
                        ...
            |-- labels_with_ids/
                |-- test/
                    |-- M-30/
                        |-- 1.txt
                        ...
                    |-- M-30-HD/
                        |-- 1.txt
                        ...
                    |-- Urban1/
                        |-- 1.txt
                        ...
                |-- train/
                    |-- M-30/
                        |-- 1001.txt
                        ...
                    |-- M-30-HD/
                        |-- 1001.txt
                        ...
                    |-- Urban1/
                        |-- 1001.txt
                        ...
        
            |-- M-30/
                |-- gt/
                    |-- gt.txt
            |-- M-30-HD/
                |-- gt/
                    |-- gt.txt
            |-- Urban1/
                |-- gt/
                    |-- gt.txt
  ```

### Installation & Run Code


In order to use this repository, you need to follow these steps:
1. Clone the GitHub repository:
```bash
git clone https://github.com/anujiisc/DeepTraffic.git
```
2. Navigate to the FairMOT directory:
```bash
cd Object\ Tracking/
cd FairMOT/
conda env create -f environment.yml
conda activate fairmot
```
3. We use DCNv2 pytorch 1.7 in our backbone network (pytorch 1.7 branch).
```
cd DCNv2
./make.sh
cd ..
```
4. You can download preprocessed GRAM and IDD dataset from . And put these datasets inside Fair-MOT/
5. Our baseline FairMOT model (DLA-34 backbone) is pretrained on the IDD(Indian Driving Dataset) for 30 epochs with the self-supervised learning approach and then trained on the GRAM dataset for 60 epochs.
6. Pretrained models are inside models/ directory eg: models/coco dla.pth .
7. Train the model using IDD dataset with the following command:
```bash
cd src/
python train.py mot --exp_id IDD --gpus 1,3 --batch_size 40 \
--load_model ../models/coco_dla.pth --num_epochs 30 --lr_step 50 \
--data_cfg ../src/lib/cfg/IDD.json
```
Adjust the arguments as required. Model weights will be saved in exp/mot/ with exp id.

8. We fine-tuned the model using the Gram dataset for an additional 60 epochs using the following command:
```bash
python train.py mot --exp_id GRAM --gpus 1,3 --batch_size 40 \
--load_model ../exp/mot/IDD/model_last.pth --num_epochs 60 --lr_step 50 \
--data_cfg ../src/lib/cfg/GRAM.json
```
Adjust the arguments as required. Model weights will be saved in exp/mot/ with exp id.